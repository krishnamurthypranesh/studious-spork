{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "import pymysql\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host = \"localhost\", user = \"pytester\", password = \"monty\", db = \"tweetsdb\",\n",
    "                            cursorclass = pymysql.cursors.DictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data\n",
    "tweets = pd.read_sql(\"select * from tweets\", connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main topic of this notebook is going to be topic modelling. I'm going to start of by looking at the tweets. Since that's what I'll be modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       RT @kryptonprobett: 🚨FREE NYE VIP PICKS🚨\\n\\nRE...\n",
       "1       #NBA \"Andre Iguodala fined for throwing ball i...\n",
       "2       Daniel Hamilton will start for Hawks vs. IND -...\n",
       "3       RT @marca: Espectacular. El mapa de la constel...\n",
       "4       @KingJames saying he winning against Steph, Kl...\n",
       "5       RT @RoyalRanksDFS: End Of The Year Special 🎉🥂 ...\n",
       "6       RT @goodformgroup: Follow the link below and t...\n",
       "7       #NFAC Closing Out 2018 w/ #BOMBS !!!\\nMonday's...\n",
       "8       RT @ToretoApuestas: Últimas 6/6 ✅\\n\\n✅ 2.00\\n✅...\n",
       "9       RT @nbastats: 📈🏀 STAT LEADERS THREAD 🏀📈\\n\\nThe...\n",
       "10      RT @NBALatam: Los @Raptors siguen teniendo el ...\n",
       "11      #MYNYEMONDAY \\n\\n#NBA: \\n\\nGrizzlies +4.5.\\nPe...\n",
       "12      RT @nbastats: The POINTS PER GAME leaders thro...\n",
       "13      Let’s close out 2018 on a high note. I discuss...\n",
       "14      RT @nbastats: The total REBOUNDS leaders throu...\n",
       "15      RT @nbastats: The REBOUNDS PER GAME leaders th...\n",
       "16      RT @nbastats: The total ASSISTS leaders throug...\n",
       "17      👳🏻‍♂️La Barba👳🏻‍♂️\\n#FreePick #NBA\\n\\n¿Quieren...\n",
       "18      RT @srsly_l: World Series to win National Leag...\n",
       "19      RT @nbastats: The ASSISTS PER GAME leaders thr...\n",
       "20      RT @nbastats: The total STEALS leaders through...\n",
       "21      RT @nbastats: The STEALS PER GAME leaders thro...\n",
       "22      RT @TheBeardBet: 👳🏻‍♂️La Barba👳🏻‍♂️\\n#FreePick...\n",
       "23      RT @SharksNSports: #MYNYEMONDAY \\n\\n#NBA: \\n\\n...\n",
       "24      RT @nbastats: The total BLOCKS leaders through...\n",
       "25      RT @nbastats: The BLOCKS PER GAME leaders thro...\n",
       "26      RT @TheBeardBet: 👳🏻‍♂️La Barba👳🏻‍♂️\\n#FreePick...\n",
       "27      RT @TheBeardBet: 👳🏻‍♂️La Barba👳🏻‍♂️\\n#FreePick...\n",
       "28      #Suns SG Devin Booker is one of 11 #NBA player...\n",
       "29              The western conference is stacked af #NBA\n",
       "                              ...                        \n",
       "1329    BOS vs SAS Dream11 NBA Prediction, Preview, Fa...\n",
       "1330    Top 10 Pelicans Instagram posts of 2018 #nba: ...\n",
       "1331    Happy New Years Eve Everyone\\nWe will have a s...\n",
       "1332    Andre Iguodala fined for throwing ball into st...\n",
       "1333    RT @ThunderDigest: This is good, right? Also: ...\n",
       "1334    RT @Evo1S: Happy New Years Eve Everyone\\nWe wi...\n",
       "1335    RT @FOXSportsFL: \"It's hard not to be a Derric...\n",
       "1336    RT @store_usapy: Exclusivo Short de la marca a...\n",
       "1337    RT @FOXSportsFL: \"It's hard not to be a Derric...\n",
       "1338    RT @shesforOKC: Missed 8 games and still assis...\n",
       "1339    RT @CMBskillz: The MJ vs Lebron debate is gett...\n",
       "1340    RT @Mogul18Magazine: @GSWHQ @aliyahmedia @Step...\n",
       "1341    RT @NBALatam: Los @Raptors siguen teniendo el ...\n",
       "1342    On the Runway: Checkout this great new read po...\n",
       "1343    Immediate rematch! It’s a copy/paste day for #...\n",
       "1344    Se acaba el 2018. Un año en el que hemos disfr...\n",
       "1345    Jonathon Simmons (ankle) out Monday.  #LetsGoM...\n",
       "1346    Jerian Grant starting for injured DJ Augustin ...\n",
       "1347    The @OrlandoMagic starting Grant, Fournier, Is...\n",
       "1348    Mavericks reportedly unlikely to trade Dennis ...\n",
       "1349    RT @DFSCashCrew1: This is the final reminder t...\n",
       "1350    Getting ready for some roundball. Mavericks at...\n",
       "1351    RT @DraftdailyDFS: 🔥The MONDAY NBA SLAM contes...\n",
       "1352    @Lockedonsports do you see the #nba ever doing...\n",
       "1353    RT @hawksbet: FREE PICKS ARE BACK!!\\n\\n**FREE ...\n",
       "1354    Un breve resumen de lo que nos deja el 2018 en...\n",
       "1355    #RingerBestOf2018: For many #NBA players, a go...\n",
       "1356    RT @ringer: #RingerBestOf2018: For many #NBA p...\n",
       "1357    Alan Henderson finishes with 11 points in Hawk...\n",
       "1358    L. Doncic 20+ points, 2+ 3 pointers, 5 rebound...\n",
       "Name: tweet_text, Length: 1359, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"tweet_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of \"@\" mentions here. These are all people. But, this is not going to add any information to us. Then, the first thing that pops out is the sheer quantity of text. So, I'll need to do something about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why I'm going to start off by pre-processing the text. Here's the plan for pre-processing the text:\n",
    "    1. Remove punctuations.\n",
    "    2. Remove @ and # symbols.\n",
    "    3. Remove all non-alpha numeric characters.\n",
    "    4. Remove all non-English tweets.\n",
    "    5. Remove all stopwords\n",
    "    6. Tokenize the data.\n",
    "    7. Lemmatize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing all punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even looking at the other items in the list, there's one thing that can be done. There's this ugly \"RT\" that's present at the head of each tweet. I can do some feature engineering by using that to create a new variable is_retweet.\n",
    "\n",
    "But, I won't be doing since the focus of this notebook is totally different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new object to operate on\n",
    "tweet_text = tweets[\"tweet_text\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the RT at the head of each tweet\n",
    "tweet_text = tweet_text.apply(lambda x: re.sub(\"^RT\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations from the data\n",
    "tweet_text = tweet_text.apply(lambda x: re.sub(\"\\d+\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing case\n",
    "tweet_text = tweet_text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing words\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tweet_text = tweet_text.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer to lemmatize words\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# defining dictionary of stopwords\n",
    "stopWords = stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to lemmatize text, remove stopwords and remove words with only one or two characters\n",
    "def normalizeText(text):\n",
    "    normalized_text = [word for word in text if word not in stopWords and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', word)]\n",
    "    normalized_text = [lemma.lemmatize(word) for word in text]\n",
    "    normalized_text = [word for word in normalized_text if len(word) > 3]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_text.apply(lambda x: normalizeText(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's not much use in retaining words that are too short. So, I'll get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing words that have less than three characters in them\n",
    "drop_words = {word for desc in tweet_text for word in desc if len(word) <= 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(drop_words) #0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be not words that have less than four characters. The next step is to move onto words that are not English. Removing them is essential to making the topics more human understandable and better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "engWords = words.words()\n",
    "drop_words_lang = {word for tweet in tweet_text for word in tweet if word not in engWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2676"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drop_words_lang) # 2676"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are 2676 words that are not part of the standard corpus of words defined in nltk. It's safe to assume that these words might not add any information and remove them. There are about 3800 words in all the tweets combined. So, it might not be the best to remove 2.6k words. \n",
    "\n",
    "Because I don't know which method is better, I'll simply build two models to know which is the better method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since I'm already losing out on so much data, I won't filter the words by frequency. If I feel that word frequencies are screwing up the result, then I'll carry out the filter operation again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDropWords(text):\n",
    "    return [word for word in text if word not in drop_words_lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_text.apply(lambda x: removeDropWords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {word for tweet in tweet_text for word in tweet}\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are about 1120 words in the corpus. This is pretty good. It's time to train our LDA model. For this, I'll be using the gensim library in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the vocabulary of our model\n",
    "dictionary = corpora.Dictionary(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the bag of words model\n",
    "corpus = [dictionary.doc2bow(text) for text in tweet_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the model, I'll be looking at 5 topics to start off with. I'll extract 10 keywords from each topic, look at the top two topics for each tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "lda_model = models.LdaModel(corpus = corpus, num_topics = 10, id2word = dictionary, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = {\"Topic \" + str(i):[] for i in range(10)}\n",
    "\n",
    "for idx, topic in enumerate(topic_terms):\n",
    "    terms = lda_model.get_topic_terms(idx)\n",
    "    for term in terms[:10]:\n",
    "        topic_terms[topic].append(dictionary[term[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>game</td>\n",
       "      <td>pacer</td>\n",
       "      <td>left</td>\n",
       "      <td>rocket</td>\n",
       "      <td>more</td>\n",
       "      <td>spur</td>\n",
       "      <td>hornet</td>\n",
       "      <td>ultimo</td>\n",
       "      <td>this</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that</td>\n",
       "      <td>game</td>\n",
       "      <td>pacer</td>\n",
       "      <td>with</td>\n",
       "      <td>year</td>\n",
       "      <td>with</td>\n",
       "      <td>pour</td>\n",
       "      <td>parlay</td>\n",
       "      <td>year</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>their</td>\n",
       "      <td>tonight</td>\n",
       "      <td>drop</td>\n",
       "      <td>grizzly</td>\n",
       "      <td>game</td>\n",
       "      <td>year</td>\n",
       "      <td>para</td>\n",
       "      <td>thunder</td>\n",
       "      <td>with</td>\n",
       "      <td>tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>will</td>\n",
       "      <td>hawk</td>\n",
       "      <td>handed</td>\n",
       "      <td>pick</td>\n",
       "      <td>sport</td>\n",
       "      <td>pick</td>\n",
       "      <td>sport</td>\n",
       "      <td>basketball</td>\n",
       "      <td>season</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>pick</td>\n",
       "      <td>watch</td>\n",
       "      <td>free</td>\n",
       "      <td>pour</td>\n",
       "      <td>pacer</td>\n",
       "      <td>rocket</td>\n",
       "      <td>spur</td>\n",
       "      <td>basketball</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>forward</td>\n",
       "      <td>player</td>\n",
       "      <td>young</td>\n",
       "      <td>today</td>\n",
       "      <td>thunder</td>\n",
       "      <td>tonight</td>\n",
       "      <td>grizzly</td>\n",
       "      <td>pick</td>\n",
       "      <td>sport</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>recognize</td>\n",
       "      <td>year</td>\n",
       "      <td>dish</td>\n",
       "      <td>over</td>\n",
       "      <td>maverick</td>\n",
       "      <td>week</td>\n",
       "      <td>check</td>\n",
       "      <td>match</td>\n",
       "      <td>harden</td>\n",
       "      <td>warrior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tomorrow</td>\n",
       "      <td>some</td>\n",
       "      <td>hammer</td>\n",
       "      <td>tonight</td>\n",
       "      <td>news</td>\n",
       "      <td>play</td>\n",
       "      <td>encore</td>\n",
       "      <td>book</td>\n",
       "      <td>latest</td>\n",
       "      <td>season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>clipper</td>\n",
       "      <td>this</td>\n",
       "      <td>league</td>\n",
       "      <td>week</td>\n",
       "      <td>match</td>\n",
       "      <td>game</td>\n",
       "      <td>with</td>\n",
       "      <td>from</td>\n",
       "      <td>total</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pride</td>\n",
       "      <td>today</td>\n",
       "      <td>into</td>\n",
       "      <td>season</td>\n",
       "      <td>have</td>\n",
       "      <td>slate</td>\n",
       "      <td>this</td>\n",
       "      <td>game</td>\n",
       "      <td>check</td>\n",
       "      <td>road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic 0  Topic 1 Topic 2  Topic 3   Topic 4  Topic 5  Topic 6  \\\n",
       "0       game    pacer    left   rocket      more     spur   hornet   \n",
       "1       that     game   pacer     with      year     with     pour   \n",
       "2      their  tonight    drop  grizzly      game     year     para   \n",
       "3       will     hawk  handed     pick     sport     pick    sport   \n",
       "4      night     pick   watch     free      pour    pacer   rocket   \n",
       "5    forward   player   young    today   thunder  tonight  grizzly   \n",
       "6  recognize     year    dish     over  maverick     week    check   \n",
       "7   tomorrow     some  hammer  tonight      news     play   encore   \n",
       "8    clipper     this  league     week     match     game     with   \n",
       "9      pride    today    into   season      have    slate     this   \n",
       "\n",
       "      Topic 7     Topic 8  Topic 9  \n",
       "0      ultimo        this     game  \n",
       "1      parlay        year     with  \n",
       "2     thunder        with  tonight  \n",
       "3  basketball      season     play  \n",
       "4        spur  basketball     week  \n",
       "5        pick       sport     that  \n",
       "6       match      harden  warrior  \n",
       "7        book      latest   season  \n",
       "8        from       total     last  \n",
       "9        game       check     road  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(topic_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the terms in each topic, I'll look into tagging each user based on the topics. \n",
    "After I do this rather simple process, I'll look into getting more data from the distribution of the topics over the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics for every user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_topics = {idx: [] for idx in tweet_text.index}\n",
    "for idx, tweet in enumerate(tweet_text):\n",
    "    bow_tweet = dictionary.doc2bow(tweet)\n",
    "    topics = sorted(lda_model.get_document_topics(bow_tweet), key = lambda x: x[1], reverse = True)[:2]\n",
    "    for topic in topics:\n",
    "        tweet_topics[idx].append(\"Topic \" + str(topic[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_topics = pd.Series(tweet_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"tweet_topics\"] = tweet_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering documents by their topic distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to cluster those tweets that are similar to each other. To do this, I'll use the Jensen-Shannon divergence as a measure of how different two probability distributions are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that instead of looking at just the top two or three tags, I'll need to look at the entire distribution of the topics to determine the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the complete distributions for each tweet\n",
    "tweets_distribution = {idx: [] for idx in tweet_text.index}\n",
    "\n",
    "for idx, tweet in enumerate(tweet_text):\n",
    "    bow_tweet = dictionary.doc2bow(tweet)\n",
    "    topics = lda_model.get_document_topics(bow_tweet)\n",
    "    for topic in topics:\n",
    "        tweets_distribution[idx].append(topic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the tweet distributions, I need calculate the JSD for each and every pair of tweets. Let's get to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before I can do that, I have to deal with 97 indexes that do not have a distribution value of 10. This is going to be a pain to fix this. So, I'll do it tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
