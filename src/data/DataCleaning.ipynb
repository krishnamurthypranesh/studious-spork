{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Writing to DB\n",
    "This file is completely for preliminary data cleaning and writing to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pacakges\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names for tweets and users\n",
    "tweets_cols = [\"tweet_id\", \"tweet_time\", \"handle\", \"location\", \"latitude\", \"longitude\", \"tweet_text\", \"media\",\n",
    "              \"favs\", \"retweets\"]\n",
    "\n",
    "users_cols = [\"name\", \"handle\", \"language\", \"verified\", \"followers\", \"following\", \"statuses\", \"favourites\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in data\n",
    "# tweets\n",
    "tweets = pd.read_csv(\"../../Data/tweet.csv\", names =  tweets_cols)\n",
    "\n",
    "# users\n",
    "users = pd.read_csv(\"../../Data/users.csv\", names = users_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets information\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the latitude and longitude contain very little information, it's better to drop 'em."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop([\"latitude\", \"longitude\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweet_id contains just the id of the tweet. tweet time is a combination of the time at and date on which the tweet was posted. Separating the two would facilitate in visualizing the tweets by time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting time and date from tweets\n",
    "created_at = [i[1] + \"-\" + i[2] + \"-\" + i[5] + \" \" + i[3] for i in tweets[\"tweet_time\"].str.split()]\n",
    "\n",
    "tweets[\"created_at\"] = pd.to_datetime(pd.Series(created_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the time and date are available, it's possible to focus on the other stuff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location variable is one thing that needs to be cleaned a lot. A look at that will tell what needs to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the places are redundant. \"CA\" stands for california. But, it's been encoded as a separate entity from USA. While this is better when it comes to visualizing tweets over location, it's better to have a single location to point to than to have multiple things for the same location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What needs to be done is this: Get a separate country variable, and based on the name of the location, populate the country variable. For this, the geopy library will come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the following chunk was run only once to get the coordinates.\n",
    "\n",
    "# from geopy import geocoders\n",
    "# locator = geocoders.Nominatim()\n",
    "\n",
    "# coordinates = {\"location\": [], \"latitude\": [], \"longitude\": []}\n",
    "\n",
    "# for user_location  in tweets[\"location\"]:\n",
    "#     try:\n",
    "#         location = locator.geocode(user_location)\n",
    "#         if location:\n",
    "#             coordinates[\"location\"].append(user_location)\n",
    "#             coordinates[\"latitude\"].append(location.latitude)\n",
    "#             coordinates[\"longitude\"].append(location.longitude)\n",
    "#     except Exception as err:\n",
    "#         pass\n",
    "\n",
    "# coordinates_df = pd.DataFrame(coordinates)\n",
    "# coordinates_df.to_csv(\"../../Data/coordinates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coordinates will be pretty helpful in visualizing the tweet counts over locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to write this data to a database so that it'll be safe and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the cleaned files to the disk\n",
    "tweets.to_csv(\"../../Data/tweetsCleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting users.verified to int\n",
    "users.loc[users.verified, \"verified\"] = 1\n",
    "users.loc[users.verified == \"False\", \"verified\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a database titled tweetsdb. The next step is to connect to it, create the tables and write the data to it. I'll use an sql script to create a database on my machine. The script can be found in src/data/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymysql.converters.encoders[np.int64] = pymysql.converters.escape_int\n",
    "pymysql.converters.conversions = pymysql.converters.encoders.copy()\n",
    "pymysql.converters.conversions.update(pymysql.converters.decoders)\n",
    "\n",
    "pymysql.converters.encoders[np.datetime64] = pymysql.converters.escape_datetime\n",
    "pymysql.converters.conversions = pymysql.converters.encoders.copy()\n",
    "pymysql.converters.conversions.update(pymysql.converters.decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a connection object\n",
    "connection = pymysql.connect(host = \"localhost\", user = \"pytester\", password = \"monty\", db = \"twitterdb\",\n",
    "                            cursorclass = pymysql.cursors.DictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets.to_sql(name = \"tweets\", con = connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two tables in tweetsdb: tweets and user_info. I'll be writing the tweets to tweets and users to user_info. I'll use two separate sql statements to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sql statement to get the data into the db\n",
    "sql_tweets = \"insert into tweets (tweet_id, tweet_time, handle, location, tweet_text, media, favs, retweets)\\\n",
    "values (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "sql_users = \"insert into user_info (name, handle, lang, verified_account, followers_count,\\\n",
    "friends_count, statuses_count, favourites_count) values (%s, %s, %s, %s, %s, %s, %s, %s)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns from which the info has to be fed to the table\n",
    "tweet_col_nums = [0, 8, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid errors, it's better to convert created_at to an object and then write it to the db. This way, the data can be broken down and analyzed as required using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"created_at\"] = pd.to_datetime(tweets[\"created_at\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets.location.isnull(), \"location\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to the tweets table\n",
    "for i in range(tweets.shape[0]):\n",
    "    try:\n",
    "        connection = pymysql.connect(host = \"localhost\", user = \"pytester\", password = \"monty\", db = \"tweetsdb\",\n",
    "                            cursorclass = pymysql.cursors.DictCursor)\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(sql_tweets, tweets.iloc[i, tweet_col_nums].tolist())\n",
    "        connection.commit()\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to the user_info table:\n",
    "for i in range(users.shape[0]):\n",
    "    try:\n",
    "        connection = pymysql.connect(host = \"localhost\", user = \"pytester\", password = \"monty\", db = \"tweetsdb\",\n",
    "                            cursorclass = pymysql.cursors.DictCursor)\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(sql_users, users.iloc[i, :].tolist())\n",
    "        connection.commit()\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        connection.close()\n",
    "    finally:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this the data have been written to the database. All required measurements are in ~src/scripts/measurements.sql file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the visualizations go to src/scripts/DataAnalysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
